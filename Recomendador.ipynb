{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup EJECUCIÓN UNICA. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import nltk y descargar stopwords de la pestaña de corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Descargamos las stopwords desde nltk corpus (Abrira una ventana nueva.)\n",
    "# Tenemos que ir a la pestaña de CORPUS, buscar la parte de las stopwords y darle descargar.  \n",
    "if __name__ == '__main__':\n",
    "    import nltk\n",
    "    nltk.download()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos que importat parser desde tika. Para esto debemos instalar tika usando  \n",
    " $pip install tika  \n",
    "Para la instalación debemos contar con JDK\n",
    "\n",
    "**NOTA** No es necesario correr la celda de abajo ya que en el proyecto que se envio ya convertimos los pdfs a txt.  \n",
    "Tampoco es necesario instalar tika a menos que se quiera volver a convertir los pdf en txt. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from tika import parser #pip install tika REQUIERE JDK\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "import unidecode #pip install unidecode\n",
    "import time\n",
    "#Import TfIdfVectorizer from scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para nuestro proyecto bajamos los PDF de los planes de cada materia de la carrera de computación.  \n",
    "Esta sección parsea los datos de los pdf a un formato más manejable, archivos de texto (.txt).  \n",
    "Estos estan en la carpeta \"TXT/\"\n",
    "\n",
    "**NOTA**: no es necesario correr la celda de abajo ya que en el proyecto que se envio ya convertimos los pdfs a txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1089.pdf\n",
      "1323.pdf\n",
      "1326.pdf\n",
      "1425.pdf\n",
      "1427.pdf\n",
      "1428.pdf\n",
      "1429.pdf\n",
      "1506.pdf\n",
      "1613.pdf\n",
      "1827.pdf\n",
      "1828.pdf\n",
      "1829.pdf\n",
      "249.pdf\n",
      "269.pdf\n",
      "277.pdf\n",
      "291.pdf\n",
      "351.pdf\n",
      "442.pdf\n",
      "446.pdf\n",
      "447.pdf\n",
      "5.pdf\n",
      "625.pdf\n",
      "626.pdf\n",
      "668.pdf\n",
      "669.pdf\n",
      "670.pdf\n",
      "671.pdf\n",
      "672.pdf\n",
      "673.pdf\n",
      "674.pdf\n",
      "676.pdf\n",
      "714.pdf\n",
      "759.pdf\n",
      "760.pdf\n",
      "764.pdf\n",
      "765.pdf\n",
      "777.pdf\n",
      "779.pdf\n",
      "783.pdf\n",
      "784.pdf\n",
      "785.pdf\n",
      "786.pdf\n",
      "787.pdf\n",
      "788.pdf\n",
      "789.pdf\n",
      "790.pdf\n",
      "791.pdf\n",
      "792.pdf\n",
      "793.pdf\n",
      "794.pdf\n",
      "803.pdf\n",
      "805.pdf\n",
      "809.pdf\n",
      "817.pdf\n",
      "819.pdf\n",
      "826.pdf\n",
      "840.pdf\n"
     ]
    }
   ],
   "source": [
    "#no es necesario correr esta sección ya que en el proyecto que se envio ya convertimos los pdfs a txt\n",
    "\n",
    "directory = \"pdf/\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".pdf\"): \n",
    "            print(filename)\n",
    "            raw = parser.from_file(os.path.join(directory, filename))\n",
    "            #print(raw['content'])\n",
    "            lines = raw['content']\n",
    "\n",
    "            lines = unidecode.unidecode(lines[77:])\n",
    "            #print(lines)\n",
    "            f = open(os.path.join(\"TXT/\", filename[:-3]+ \"txt\"), 'w')\n",
    "            f.write(lines)\n",
    "            f.close()\n",
    "            time.sleep(.9)\n",
    "            continue\n",
    "        else:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aqui empieza el main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El principal problema para la creacion de nuestro recomendador es el Natural Language Processing o procesamiento de lenguaje natural .  \n",
    "Para extraer caracteristicas y similitudes entre los documentos, vamos a calcular los vectores de palabras de cada uno de estos.  \n",
    "Los vectores de palabras son representaciones vectoriales de las palabras de un documento.\n",
    "\n",
    "Para crearlos vamos a utilizar el Term Frequency-Inverse Document Frequency (TF-IDF). Esto nos devolvera una matriz donde cada columna represente una palabra en nuestro vocabulario, y cada columna representara un plan de estudios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import unidecode #pip install unidecode\n",
    "import time\n",
    "#Import TfIdfVectorizer from scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "directory = \"data/planes_materias/TXT/\"\n",
    "\n",
    "materias = {}\n",
    "#print(\"hola\")\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        #print(filename) \n",
    "        f = open(os.path.join(directory, filename), 'r')\n",
    "        materias[filename[:-4]] = f.read()\n",
    "\n",
    "#print(materias['1.txt'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(stop_words=['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los',\n",
       "                            'del', 'se', 'las', 'por', 'un', 'para', 'con',\n",
       "                            'no', 'una', 'su', 'al', 'lo', 'como', 'más',\n",
       "                            'pero', 'sus', 'le', 'ya', 'o', 'este', 'sí',\n",
       "                            'porque', ...])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = stopwords.words('spanish')\n",
    "my_stop_words = [\"CARRERA\",\"MATEMATICO\",\"SERIACION\", \"INDICATIVA\" ,\"ANTECEDENTE\",\"MODALIDAD\",\n",
    "                \"CURSO\",\"CARACTER\", \"TEORICAS\", \"PRACTICAS\" ,\"CREDITOS\", \"HORAS\", \"SEMANA\", \"SEMESTRE\", \"CLAVE\",\n",
    "                \"FACULTAD\", \"CIENCIAS\", \"OBJETIVO\",\"BIBLIOGRAFIA\", \"BASICA\", \"COMPLEMENTARIA\",\"SUGERENCIA\",           \n",
    "                \"PARA\",\"LA\",\"EVALUACION\",\"DE\",\"LA\",\"ASIGNATURA\",\"Ademas\",\"de\",\"las\",\"calificaciones\",\n",
    "                \"en\",\"examenes\",\"y\",\"tareas\",\"se\",\"tomara\",\"en\",\"cuenta\",\"la\",\"participacion\",\"del\",\"alumno\",\n",
    "                \"PERFIL\",\"PROFESIOGRAFICO\",\"Matematico\",\"fisico\",\"actuario\",\"o\",\"licenciado\",\"en\",\"ciencias\",\n",
    "                \"de\",\"la\",\"computacion\",\"especialista\",\"en\",\"el\",\"area\",\"de\",\"la\",\"asignatura\",\"a\",\"juicio\",\n",
    "                \"del\",\"comite\",\"de\",\"asignacion\",\"de\", \"cursos\",\"UNIDADES\", \"TEMATICAS\",\"SERIACION\" ,\n",
    "                \"INDICATIVA\", \"ANTECEDENTE\", \"SUBSECUENTE\", \"optativo\", \"Optativas\", \"VERSIDAD\", \"Computacion\", \"el\",\n",
    "                \"plan\", \"y\", \"programa\", \"de\", \"estudios\", \"de\", \"la\", \"licenciatura\", \"en\", \"Ciencias\", \"Computacion\",\n",
    "                 \">Ciencias\", \n",
    "                ]\n",
    "stop_words.append(my_stop_words)\n",
    "TfidfVectorizer(stop_words=stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(intereses):\n",
    "    #Palabras que no consideraremos.\n",
    "    stop_words = stopwords.words('spanish')\n",
    "    my_stop_words = [\"CARRERA\",\"MATEMATICO\",\"SERIACION\", \"INDICATIVA\" ,\"ANTECEDENTE\",\"MODALIDAD\",\n",
    "    \"CURSO\",\"CARACTER\", \"TEORICAS\", \"PRACTICAS\" ,\"CREDITOS\", \"HORAS\", \"SEMANA\", \"SEMESTRE\", \"CLAVE\",\n",
    "    \"FACULTAD\", \"CIENCIAS\", \"OBJETIVO\",\"BIBLIOGRAFIA\", \"BASICA\", \"COMPLEMENTARIA\",\"SUGERENCIA\",           \"PARA\",\"LA\",\"EVALUACION\",\"DE\",\"LA\",\"ASIGNATURA\",\"Ademas\",\"de\",\"las\",\"calificaciones\",\n",
    "    \"en\",\"examenes\",\"y\",\"tareas\",\"se\",\"tomara\",\"en\",\"cuenta\",\"la\",\"participacion\",\"del\",\"alumno\",\n",
    "    \"PERFIL\",\"PROFESIOGRAFICO\",\"Matematico\",\"fisico\",\"actuario\",\"o\",\"licenciado\",\"en\",\"ciencias\",\n",
    "    \"de\",\"la\",\"computacion\",\"especialista\",\"en\",\"el\",\"area\",\"de\",\"la\",\"asignatura\",\"a\",\"juicio\",\n",
    "    \"del\",\"comite\",\"de\",\"asignacion\",\"de\", \"cursos\",\"UNIDADES\", \"TEMATICAS\",\"SERIACION\" ,\n",
    "    \"INDICATIVA\" ,\"ANTECEDENTE\", \"SUBSECUENTE\",\"optativo\",\"Optativas\"]\n",
    "    stop_words.append(my_stop_words)\n",
    "    materias[0] = intereses\n",
    "    #data = pd.DataFrame(materias)\n",
    "    data = pd.Series(materias).to_frame()\n",
    "    type(data)\n",
    "    data = data.rename(columns={0: 'Planes'})\n",
    "    #data[\"index\"] = pd.to_numeric(data[\"index\"])\n",
    "    data.index = data.index.map(int)\n",
    "    data.sort_index(inplace=True)\n",
    "\n",
    "    #print(data.columns)\n",
    "    #print(data.index)\n",
    "\n",
    "\n",
    "    #Define a TF-IDF Vectorizer Object. Remove all spanis stop words such as 'de', 'la', 'que', 'el'\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        tfidf = TfidfVectorizer(stop_words)\n",
    "\n",
    "    #Construct the required TF-IDF matrix by fitting and transforming the data\n",
    "    #print(data['Planes'])\n",
    "    tfidf_matrix = tfidf.fit_transform(data['Planes'])\n",
    "\n",
    "    #Output the shape of tfidf_matrix\n",
    "    tfidf_matrix.shape\n",
    "\n",
    "    #Palabras encontradas en todos los archivos\n",
    "    tfidf.get_feature_names()[2990:3007]\n",
    "\n",
    "\n",
    "    # Compute the cosine similarity matrix\n",
    "    cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "    (cosine_sim.shape)\n",
    "    #print(cosine_sim[1])\n",
    "    \n",
    "    return data, cosine_sim\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#indices = pd.Series(data.index, index=data.index).drop_duplicates()\n",
    "\n",
    "# Function that takes in movie title as input and outputs most similar movies\n",
    "def get_recommendations(intereses):\n",
    "    # Get the index of the movie that matches the title\n",
    "    data, matriz_similitudes = train(intereses)\n",
    "\n",
    "    # Get the pairwsie similarity scores of all plans with la lista\n",
    "    sim_scores = list(enumerate(matriz_similitudes[0]))\n",
    "    #print(sim_scores)\n",
    "\n",
    "    # Sort the topics based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    #print(sim_scores)\n",
    "    # Get the scores of the  10most similar topics\n",
    "    sim_scores = sim_scores[0:11]\n",
    "\n",
    "    # Get the movie indices\n",
    "    recomendaciones = [i[0] for i in sim_scores]\n",
    "    #print(recomendaciones)\n",
    "\n",
    "    # Return the top 10 most similar topics\n",
    "    return list(data.iloc[recomendaciones].index[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[162, 765, 183, 165, 766, 395, 433, 163, 712, 939]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Devuelve una lista de 10 recomendaciones de la posicion 5 de los temarios. \n",
    "\n",
    "# Poner posicion, no clave.\n",
    "#1.pdf -> 0\n",
    "#2.pdf -> 1\n",
    "\n",
    "\"\"\"\n",
    "recomendacion = get_recommendations(\"Ecuaciones diferenciales\")\n",
    "recomendacion\n",
    "\"\"\"\n",
    "\"\"\"recomendacion = get_recommendations(\"topologia ecuaciones diferenciales\")\n",
    "recomendacion\n",
    "\"\"\"\n",
    "get_recommendations('topologia ecuaciones diferenciales')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recomendacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
